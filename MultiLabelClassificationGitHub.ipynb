{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultiLabelClassificationGitHub",
      "provenance": [],
      "collapsed_sections": [
        "elczM9jRnUeG",
        "iIAogNLBpYIf",
        "WW5NR5gQczxn",
        "jMOw3BkM5d0Y",
        "NnPJdgqrWRkY"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE3Hb0BvnFuv",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "Created on Thu May 7 2020 <br>\n",
        "@author: goutham <br>\n",
        "\n",
        "## Multi Label Classifcation using keras Deep Neural Networks <br>\n",
        "\n",
        "Topics\n",
        "* Keras Neural Networks\n",
        "* Multi Label Classification \n",
        "* Metric = 'accuracy'\n",
        "* Model Evaluation using Custom Metric <br>\n",
        "\n",
        "Packages Used: \n",
        "* keras\n",
        "* numpy\n",
        "* pandas <br>\n",
        "\n",
        "The purpose of this code is to show how one can build a basic Multi Label Classification Neural Network using Keras.\n",
        "This code also helps one understand what to look out for when using 'accuracy' as a metric for model evaluation in multi label classification problems with high target dimensionality and sparse data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "elczM9jRnUeG"
      },
      "source": [
        "## Get the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zgBUqNP_nUeT",
        "colab": {}
      },
      "source": [
        "# Input the Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "dataGroup = pd.read_csv('YourPath/MLCSampleData.csv') # Insert your data path here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g2ND7ESaKSB",
        "colab_type": "code",
        "outputId": "f1b2491d-e62a-4086-f85b-8ba16f328989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "# Understand the Data\n",
        "dataGroup.head()\n",
        "# This is dummy data that I created to use for building a multi label classification keras neural network\n",
        "# The label data is one-hot encoded - the way it needs to be fed into the output layer\n",
        "# id column is a unique identifier for an instance \n",
        "# Feature0 to Feature99 are the 100 features that should be used to build the classifier\n",
        "# Label0 to Label39 are the 40 distinct labels (output) available and the data is sparse"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Feature0</th>\n",
              "      <th>Feature1</th>\n",
              "      <th>Feature2</th>\n",
              "      <th>Feature3</th>\n",
              "      <th>Feature4</th>\n",
              "      <th>Feature5</th>\n",
              "      <th>Feature6</th>\n",
              "      <th>Feature7</th>\n",
              "      <th>Feature8</th>\n",
              "      <th>Feature9</th>\n",
              "      <th>Feature10</th>\n",
              "      <th>Feature11</th>\n",
              "      <th>Feature12</th>\n",
              "      <th>Feature13</th>\n",
              "      <th>Feature14</th>\n",
              "      <th>Feature15</th>\n",
              "      <th>Feature16</th>\n",
              "      <th>Feature17</th>\n",
              "      <th>Feature18</th>\n",
              "      <th>Feature19</th>\n",
              "      <th>Feature20</th>\n",
              "      <th>Feature21</th>\n",
              "      <th>Feature22</th>\n",
              "      <th>Feature23</th>\n",
              "      <th>Feature24</th>\n",
              "      <th>Feature25</th>\n",
              "      <th>Feature26</th>\n",
              "      <th>Feature27</th>\n",
              "      <th>Feature28</th>\n",
              "      <th>Feature29</th>\n",
              "      <th>Feature30</th>\n",
              "      <th>Feature31</th>\n",
              "      <th>Feature32</th>\n",
              "      <th>Feature33</th>\n",
              "      <th>Feature34</th>\n",
              "      <th>Feature35</th>\n",
              "      <th>Feature36</th>\n",
              "      <th>Feature37</th>\n",
              "      <th>Feature38</th>\n",
              "      <th>...</th>\n",
              "      <th>Label0</th>\n",
              "      <th>Label1</th>\n",
              "      <th>Label2</th>\n",
              "      <th>Label3</th>\n",
              "      <th>Label4</th>\n",
              "      <th>Label5</th>\n",
              "      <th>Label6</th>\n",
              "      <th>Label7</th>\n",
              "      <th>Label8</th>\n",
              "      <th>Label9</th>\n",
              "      <th>Label10</th>\n",
              "      <th>Label11</th>\n",
              "      <th>Label12</th>\n",
              "      <th>Label13</th>\n",
              "      <th>Label14</th>\n",
              "      <th>Label15</th>\n",
              "      <th>Label16</th>\n",
              "      <th>Label17</th>\n",
              "      <th>Label18</th>\n",
              "      <th>Label19</th>\n",
              "      <th>Label20</th>\n",
              "      <th>Label21</th>\n",
              "      <th>Label22</th>\n",
              "      <th>Label23</th>\n",
              "      <th>Label24</th>\n",
              "      <th>Label25</th>\n",
              "      <th>Label26</th>\n",
              "      <th>Label27</th>\n",
              "      <th>Label28</th>\n",
              "      <th>Label29</th>\n",
              "      <th>Label30</th>\n",
              "      <th>Label31</th>\n",
              "      <th>Label32</th>\n",
              "      <th>Label33</th>\n",
              "      <th>Label34</th>\n",
              "      <th>Label35</th>\n",
              "      <th>Label36</th>\n",
              "      <th>Label37</th>\n",
              "      <th>Label38</th>\n",
              "      <th>Label39</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.015835</td>\n",
              "      <td>-0.045127</td>\n",
              "      <td>0.094640</td>\n",
              "      <td>0.048653</td>\n",
              "      <td>-0.066024</td>\n",
              "      <td>-0.023271</td>\n",
              "      <td>-0.016250</td>\n",
              "      <td>0.033512</td>\n",
              "      <td>0.049166</td>\n",
              "      <td>-0.013469</td>\n",
              "      <td>0.022523</td>\n",
              "      <td>-0.079664</td>\n",
              "      <td>0.012362</td>\n",
              "      <td>-0.005313</td>\n",
              "      <td>0.016847</td>\n",
              "      <td>-0.007048</td>\n",
              "      <td>-0.014908</td>\n",
              "      <td>0.021623</td>\n",
              "      <td>0.071576</td>\n",
              "      <td>-0.024167</td>\n",
              "      <td>0.017272</td>\n",
              "      <td>-0.052168</td>\n",
              "      <td>-0.050011</td>\n",
              "      <td>-0.044022</td>\n",
              "      <td>-0.032369</td>\n",
              "      <td>0.028092</td>\n",
              "      <td>0.025928</td>\n",
              "      <td>0.038884</td>\n",
              "      <td>-0.050378</td>\n",
              "      <td>-0.056456</td>\n",
              "      <td>0.040243</td>\n",
              "      <td>-0.008851</td>\n",
              "      <td>0.042443</td>\n",
              "      <td>0.053557</td>\n",
              "      <td>-0.019753</td>\n",
              "      <td>0.049580</td>\n",
              "      <td>0.025410</td>\n",
              "      <td>-0.001016</td>\n",
              "      <td>-0.024046</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.063157</td>\n",
              "      <td>-0.017947</td>\n",
              "      <td>-0.030838</td>\n",
              "      <td>-0.007913</td>\n",
              "      <td>-0.028523</td>\n",
              "      <td>-0.001570</td>\n",
              "      <td>-0.002896</td>\n",
              "      <td>0.037894</td>\n",
              "      <td>0.095409</td>\n",
              "      <td>0.044269</td>\n",
              "      <td>-0.010836</td>\n",
              "      <td>-0.000652</td>\n",
              "      <td>-0.004343</td>\n",
              "      <td>-0.032067</td>\n",
              "      <td>-0.015429</td>\n",
              "      <td>0.033184</td>\n",
              "      <td>-0.020228</td>\n",
              "      <td>-0.005302</td>\n",
              "      <td>0.051265</td>\n",
              "      <td>0.037860</td>\n",
              "      <td>-0.047716</td>\n",
              "      <td>-0.017397</td>\n",
              "      <td>0.054960</td>\n",
              "      <td>-0.036262</td>\n",
              "      <td>0.021581</td>\n",
              "      <td>-0.057065</td>\n",
              "      <td>0.012647</td>\n",
              "      <td>-0.084299</td>\n",
              "      <td>0.008484</td>\n",
              "      <td>0.031245</td>\n",
              "      <td>-0.003681</td>\n",
              "      <td>0.058182</td>\n",
              "      <td>0.009452</td>\n",
              "      <td>-0.041775</td>\n",
              "      <td>0.007769</td>\n",
              "      <td>0.047030</td>\n",
              "      <td>0.036144</td>\n",
              "      <td>0.013338</td>\n",
              "      <td>0.029696</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.037002</td>\n",
              "      <td>-0.030174</td>\n",
              "      <td>0.080122</td>\n",
              "      <td>-0.001855</td>\n",
              "      <td>-0.013632</td>\n",
              "      <td>-0.009644</td>\n",
              "      <td>-0.021854</td>\n",
              "      <td>-0.017841</td>\n",
              "      <td>-0.012354</td>\n",
              "      <td>-0.110699</td>\n",
              "      <td>-0.005091</td>\n",
              "      <td>-0.032056</td>\n",
              "      <td>0.028236</td>\n",
              "      <td>-0.040865</td>\n",
              "      <td>-0.016642</td>\n",
              "      <td>0.051011</td>\n",
              "      <td>-0.014348</td>\n",
              "      <td>-0.004535</td>\n",
              "      <td>0.069596</td>\n",
              "      <td>-0.018855</td>\n",
              "      <td>-0.029222</td>\n",
              "      <td>-0.061583</td>\n",
              "      <td>-0.028351</td>\n",
              "      <td>-0.038448</td>\n",
              "      <td>-0.056914</td>\n",
              "      <td>0.024536</td>\n",
              "      <td>0.052116</td>\n",
              "      <td>-0.072196</td>\n",
              "      <td>-0.044240</td>\n",
              "      <td>-0.045987</td>\n",
              "      <td>0.025957</td>\n",
              "      <td>-0.066354</td>\n",
              "      <td>0.038996</td>\n",
              "      <td>-0.042534</td>\n",
              "      <td>0.018681</td>\n",
              "      <td>0.072701</td>\n",
              "      <td>-0.055200</td>\n",
              "      <td>0.036336</td>\n",
              "      <td>-0.049919</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.041543</td>\n",
              "      <td>-0.020182</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>-0.080072</td>\n",
              "      <td>0.032774</td>\n",
              "      <td>-0.004889</td>\n",
              "      <td>0.036260</td>\n",
              "      <td>0.047002</td>\n",
              "      <td>0.003048</td>\n",
              "      <td>-0.036937</td>\n",
              "      <td>-0.066972</td>\n",
              "      <td>0.051171</td>\n",
              "      <td>0.047988</td>\n",
              "      <td>0.058928</td>\n",
              "      <td>-0.054135</td>\n",
              "      <td>0.013575</td>\n",
              "      <td>-0.017803</td>\n",
              "      <td>-0.008322</td>\n",
              "      <td>-0.040067</td>\n",
              "      <td>-0.023788</td>\n",
              "      <td>0.042533</td>\n",
              "      <td>-0.045643</td>\n",
              "      <td>0.016811</td>\n",
              "      <td>-0.039583</td>\n",
              "      <td>0.023409</td>\n",
              "      <td>-0.097692</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>-0.036811</td>\n",
              "      <td>-0.006163</td>\n",
              "      <td>0.007077</td>\n",
              "      <td>0.059484</td>\n",
              "      <td>0.050211</td>\n",
              "      <td>0.025730</td>\n",
              "      <td>-0.071290</td>\n",
              "      <td>0.046276</td>\n",
              "      <td>0.032288</td>\n",
              "      <td>0.102312</td>\n",
              "      <td>0.016655</td>\n",
              "      <td>-0.021959</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>-0.041543</td>\n",
              "      <td>-0.020182</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>-0.080072</td>\n",
              "      <td>0.032774</td>\n",
              "      <td>-0.004889</td>\n",
              "      <td>0.036260</td>\n",
              "      <td>0.047002</td>\n",
              "      <td>0.003048</td>\n",
              "      <td>-0.036937</td>\n",
              "      <td>-0.066972</td>\n",
              "      <td>0.051171</td>\n",
              "      <td>0.047988</td>\n",
              "      <td>0.058928</td>\n",
              "      <td>-0.054135</td>\n",
              "      <td>0.013575</td>\n",
              "      <td>-0.017803</td>\n",
              "      <td>-0.008322</td>\n",
              "      <td>-0.040067</td>\n",
              "      <td>-0.023788</td>\n",
              "      <td>0.042533</td>\n",
              "      <td>-0.045643</td>\n",
              "      <td>0.016811</td>\n",
              "      <td>-0.039583</td>\n",
              "      <td>0.023409</td>\n",
              "      <td>-0.097692</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>-0.036811</td>\n",
              "      <td>-0.006163</td>\n",
              "      <td>0.007077</td>\n",
              "      <td>0.059484</td>\n",
              "      <td>0.050211</td>\n",
              "      <td>0.025730</td>\n",
              "      <td>-0.071290</td>\n",
              "      <td>0.046276</td>\n",
              "      <td>0.032288</td>\n",
              "      <td>0.102312</td>\n",
              "      <td>0.016655</td>\n",
              "      <td>-0.021959</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 141 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  Feature0  Feature1  Feature2  ...  Label36  Label37  Label38  Label39\n",
              "0   1  0.015835 -0.045127  0.094640  ...        0        0        0        0\n",
              "1   2 -0.063157 -0.017947 -0.030838  ...        0        0        0        0\n",
              "2   3 -0.037002 -0.030174  0.080122  ...        0        0        0        1\n",
              "3   4 -0.041543 -0.020182  0.002726  ...        0        0        0        0\n",
              "4   5 -0.041543 -0.020182  0.002726  ...        0        0        0        0\n",
              "\n",
              "[5 rows x 141 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj_LeM2Jx7qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Test Split : Random Split 90:10 # Usually one does 80:20\n",
        "X = dataGroup.iloc[:,1:101].copy() # get Feature0 to Feature99\n",
        "y = dataGroup.iloc[:,101:].copy() # get Label0 to Label39\n",
        "from sklearn.model_selection import train_test_split # random sampling split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0) # using random_state=0 for replication of results # should not use it when deploying\n",
        "\n",
        "# transform the (Y) target into a numy array\n",
        "y_train_list = y_train.values.tolist()\n",
        "y_train = np.array(y_train_list)\n",
        "\n",
        "y_test_list = y_test.values.tolist()\n",
        "y_test = np.array(y_test_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIAogNLBpYIf",
        "colab_type": "text"
      },
      "source": [
        "## Build the Classifier\n",
        "\n",
        "A very simple neural network was built as the primary objective was to focus on how the output layer and the loss function differs in a multi label classification from that of a conventional multi class classification network. <br>\n",
        "This could be used as a base to build complex multi label classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FE0YhYSokfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6dee51e6-7825-41fa-b310-1cd51f6f5930"
      },
      "source": [
        "# use keras framework for deep learning\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwvIQzQxouE4",
        "colab_type": "code",
        "outputId": "5fbca83c-2b5a-403b-d629-ac1f96ba33c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# load a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# construct the network\n",
        "model.add(Dense(units=64, activation='relu', input_dim=len(x_train.columns))) # len(x_train.columns) will automatically get the required count of nodes for input layer\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(len(y_train[0]), activation='sigmoid')) # Sigmoid over Softmax although we have multiple classes for independence across labels\n",
        "                                                        # len(y_train[0]) = number of labels = number of nodes in output layer\n",
        "\n",
        "# compile the model # using binary crossentropy over categorical crossentropy for independence across labels\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "# I'm using metrics=['accuracy']  in model.compile just to show what happens in model evaluation step later\n",
        "# If the number of distinct labels is high and the data is sparse, i would ideally compile the model using the below line\n",
        "#model.compile(loss='binary_crossentropy', optimizer='adam') \n",
        "\n",
        "# y_train is a Numpy arrays \n",
        "model.fit(x_train, y_train, epochs=150, batch_size=36, verbose = 0) # Using verbose = 0 as the interim output will make the notebook huge"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f84f7dee048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lfUowgsCbfK",
        "colab_type": "text"
      },
      "source": [
        "Let's see what the model evaluation tells us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avpRBmVsCZFo",
        "colab_type": "code",
        "outputId": "aa02c117-3585-41ea-a089-18820b4d61cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Evaluate the model on the test data using 'evaluate'\n",
        "print(\"Output when model evaluation was performed using model.evaluate() and \")\n",
        "print(\"model compiled using <model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])>\")\n",
        "print()\n",
        "results = model.evaluate(x_test, y_test, verbose = 0)\n",
        "print(\"Test Accuracy\",\"{:.2%}\".format(results[1]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output when model evaluation was performed using model.evaluate() and \n",
            "model compiled using <model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])>\n",
            "\n",
            "Test Accuracy 97.22%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imBU62EYDC4t",
        "colab_type": "text"
      },
      "source": [
        "Let's see how many records were accurately labelled by the model - our own evaluation metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5EYczFnXHK",
        "colab_type": "code",
        "outputId": "903af859-1955-4e90-f90d-4b520811bd03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# Predict on test data using the model\n",
        "labelsPredict = model.predict(x_test)\n",
        "\n",
        "# I'm going to call a label 1 if its value > 0.5 and so, im going to use round function to get labels\n",
        "# Threshold could be set dynamically for better performance of the model\n",
        "labelsPredict = labelsPredict.round()\n",
        "\n",
        "# Create Multi Label Vectors for the observed data # look at the output below to understand what this transformation does \n",
        "observed = []\n",
        "for i in range(len(y_test)):\n",
        "  multiLabel = ''\n",
        "  for j in range(len(y_test[i,:])):\n",
        "    multiLabel = multiLabel+str(y_test[i,j])\n",
        "  observed.append(multiLabel)\n",
        "print('Observed Data - first 5 records')\n",
        "print(observed[:5])\n",
        "print()\n",
        "\n",
        "# Create Multi Label Vectors for the data predicted using model 1 # look at the output below to understand what this transformation does \n",
        "predicted = []\n",
        "for i in range(len(labelsPredict)):\n",
        "  multiLabel = ''\n",
        "  for j in range(len(labelsPredict[i])):\n",
        "    multiLabel = multiLabel+str(int(labelsPredict[i,j]))\n",
        "  predicted.append(multiLabel)\n",
        "print('Predicted Data - first 5 records')\n",
        "print(predicted[:5])\n",
        "print()\n",
        "\n",
        "# For how many records did the model get all the labels correct?\n",
        "is_accurate = []\n",
        "for i in range(len(observed)):\n",
        "  is_accurate.append(observed[i]==predicted[i])\n",
        "\n",
        "CategoricalAccuracy = sum(is_accurate)/len(is_accurate) # (count of records accurately tagged)/(total number of records)\n",
        "print('% of test records for which the model got all the labels correct is')\n",
        "print(\"{:.2%}\".format(CategoricalAccuracy))  \n",
        "print()\n",
        "print(\"total number of test records:\",len(is_accurate))\n",
        "print(\"accurately predicted records:\",sum(is_accurate))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observed Data - first 5 records\n",
            "['0000000000000000000000100000000000000000', '0000000001000000000000000000000000000000', '0000010000000000000000000000000000000000', '0000000000000000000000000000000000000100', '0000000000000000000000100000000000000000']\n",
            "\n",
            "Predicted Data - first 5 records\n",
            "['0000000000000100000000000000000000010000', '0000000000000000000000000000000000000000', '0000000000000000000000000000000000000000', '0000000000000000000001000000000000000000', '0000000000000000000000100000000000000000']\n",
            "\n",
            "% of test records for which the model got all the labels correct is\n",
            "30.16%\n",
            "\n",
            "total number of test records: 63\n",
            "accurately predicted records: 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb8ltG6CL692",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the code in the above cell calls the predictions for a record accurate only if the model gets all the labels correct for that record. But, the keras default accuracy metric considers each prediction as an individual event and so the discrepancy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX8bMOoD8cfG",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "There is **nothing wrong** with the keras accuracy metric. <br>\n",
        "It is working the way it is intended to work. It considers each record-label prediction as an individual event.<br>\n",
        "I've written a story on Medium explaining how 'Accuracy' works for a multi label classification problem and how to avoid any pitfalls that may arise from misinterpreting it.\n",
        "Please refer to it for a non technical explanation on why the model evaluation is giving a test accuracy of 97% when the model only got multi-labels correct for 30% of the data."
      ]
    }
  ]
}
